{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "157/157 [==============================] - 32s 194ms/step - loss: 0.4894 - accuracy: 0.7506 - val_loss: 0.3241 - val_accuracy: 0.8696\n",
      "Epoch 2/20\n",
      "157/157 [==============================] - 28s 178ms/step - loss: 0.2512 - accuracy: 0.9010 - val_loss: 0.3034 - val_accuracy: 0.8768\n",
      "Epoch 3/20\n",
      "157/157 [==============================] - 30s 193ms/step - loss: 0.1720 - accuracy: 0.9372 - val_loss: 0.3577 - val_accuracy: 0.8748\n",
      "Epoch 4/20\n",
      "157/157 [==============================] - 31s 195ms/step - loss: 0.1151 - accuracy: 0.9594 - val_loss: 0.3497 - val_accuracy: 0.8628\n",
      "Epoch 5/20\n",
      "157/157 [==============================] - 27s 174ms/step - loss: 0.0902 - accuracy: 0.9694 - val_loss: 0.5493 - val_accuracy: 0.8634\n",
      "Epoch 6/20\n",
      "157/157 [==============================] - 27s 175ms/step - loss: 0.0688 - accuracy: 0.9772 - val_loss: 0.4615 - val_accuracy: 0.8586\n",
      "Epoch 7/20\n",
      "157/157 [==============================] - 28s 178ms/step - loss: 0.0613 - accuracy: 0.9808 - val_loss: 0.4911 - val_accuracy: 0.8596\n",
      "Epoch 8/20\n",
      "157/157 [==============================] - 30s 189ms/step - loss: 0.0467 - accuracy: 0.9851 - val_loss: 0.5600 - val_accuracy: 0.8560\n",
      "Epoch 9/20\n",
      "157/157 [==============================] - 29s 185ms/step - loss: 0.0556 - accuracy: 0.9822 - val_loss: 0.5715 - val_accuracy: 0.8514\n",
      "Epoch 10/20\n",
      "157/157 [==============================] - 29s 186ms/step - loss: 0.0488 - accuracy: 0.9848 - val_loss: 0.5797 - val_accuracy: 0.8524\n",
      "Epoch 11/20\n",
      "157/157 [==============================] - 28s 178ms/step - loss: 0.0449 - accuracy: 0.9851 - val_loss: 0.5305 - val_accuracy: 0.8324\n",
      "Epoch 12/20\n",
      "157/157 [==============================] - 28s 175ms/step - loss: 0.0315 - accuracy: 0.9905 - val_loss: 0.7211 - val_accuracy: 0.8620\n",
      "Epoch 13/20\n",
      "157/157 [==============================] - 28s 181ms/step - loss: 0.0388 - accuracy: 0.9879 - val_loss: 0.6656 - val_accuracy: 0.8498\n",
      "Epoch 14/20\n",
      "157/157 [==============================] - 29s 182ms/step - loss: 0.0128 - accuracy: 0.9976 - val_loss: 0.6988 - val_accuracy: 0.8494\n",
      "Epoch 15/20\n",
      "157/157 [==============================] - 28s 179ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.8003 - val_accuracy: 0.8512\n",
      "Epoch 16/20\n",
      "157/157 [==============================] - 28s 178ms/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 0.8832 - val_accuracy: 0.8622\n",
      "Epoch 17/20\n",
      "157/157 [==============================] - 28s 179ms/step - loss: 0.0067 - accuracy: 0.9984 - val_loss: 0.8667 - val_accuracy: 0.7808\n",
      "Epoch 18/20\n",
      "157/157 [==============================] - 28s 181ms/step - loss: 0.0466 - accuracy: 0.9852 - val_loss: 0.7041 - val_accuracy: 0.8558\n",
      "Epoch 19/20\n",
      "157/157 [==============================] - 29s 186ms/step - loss: 0.0229 - accuracy: 0.9931 - val_loss: 0.7159 - val_accuracy: 0.8480\n",
      "Epoch 20/20\n",
      "157/157 [==============================] - 30s 189ms/step - loss: 0.0109 - accuracy: 0.9967 - val_loss: 0.7611 - val_accuracy: 0.8462\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.8298 - accuracy: 0.8390\n",
      "Test Loss: 0.8298282027244568, Test Accuracy: 0.8389599919319153\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Setting up parameters\n",
    "vocab_size = 10000  # Size of vocabulary\n",
    "max_len = 200  # Maximum length of sequences\n",
    "embedding_dim = 128  # Dimension of word embeddings\n",
    "\n",
    "# Loading the IMDB dataset\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# Preprocessing the data\n",
    "train_data = pad_sequences(train_data, maxlen=max_len)\n",
    "test_data = pad_sequences(test_data, maxlen=max_len)\n",
    "\n",
    "# Defining the model architecture\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(train_data, train_labels, epochs=20, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = model.evaluate(test_data, test_labels)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "157/157 [==============================] - 39s 240ms/step - loss: 0.5665 - accuracy: 0.6963 - val_loss: 0.4573 - val_accuracy: 0.7972\n",
      "Epoch 2/20\n",
      "157/157 [==============================] - 28s 178ms/step - loss: 0.3507 - accuracy: 0.8533 - val_loss: 0.6475 - val_accuracy: 0.7384\n",
      "Epoch 3/20\n",
      "157/157 [==============================] - 27s 172ms/step - loss: 0.2869 - accuracy: 0.8869 - val_loss: 0.4172 - val_accuracy: 0.8054\n",
      "Epoch 4/20\n",
      "157/157 [==============================] - 28s 175ms/step - loss: 0.2443 - accuracy: 0.9049 - val_loss: 0.3198 - val_accuracy: 0.8752\n",
      "Epoch 5/20\n",
      "157/157 [==============================] - 28s 175ms/step - loss: 0.2121 - accuracy: 0.9196 - val_loss: 0.4128 - val_accuracy: 0.8496\n",
      "Epoch 6/20\n",
      "157/157 [==============================] - 28s 176ms/step - loss: 0.1906 - accuracy: 0.9294 - val_loss: 0.3341 - val_accuracy: 0.8636\n",
      "Epoch 7/20\n",
      "157/157 [==============================] - 28s 177ms/step - loss: 0.1727 - accuracy: 0.9359 - val_loss: 0.3345 - val_accuracy: 0.8532\n",
      "Epoch 8/20\n",
      "157/157 [==============================] - 28s 178ms/step - loss: 0.1481 - accuracy: 0.9471 - val_loss: 0.3405 - val_accuracy: 0.8706\n",
      "Epoch 9/20\n",
      "157/157 [==============================] - 28s 180ms/step - loss: 0.1341 - accuracy: 0.9517 - val_loss: 0.3676 - val_accuracy: 0.8498\n",
      "Epoch 10/20\n",
      "157/157 [==============================] - 28s 180ms/step - loss: 0.1163 - accuracy: 0.9594 - val_loss: 0.3621 - val_accuracy: 0.8682\n",
      "Epoch 11/20\n",
      "157/157 [==============================] - 28s 181ms/step - loss: 0.1040 - accuracy: 0.9642 - val_loss: 0.4427 - val_accuracy: 0.8728\n",
      "Epoch 12/20\n",
      "157/157 [==============================] - 30s 188ms/step - loss: 0.0891 - accuracy: 0.9693 - val_loss: 0.4310 - val_accuracy: 0.8636\n",
      "Epoch 13/20\n",
      "157/157 [==============================] - 30s 188ms/step - loss: 0.0829 - accuracy: 0.9722 - val_loss: 0.4434 - val_accuracy: 0.8500\n",
      "Epoch 14/20\n",
      "157/157 [==============================] - 30s 189ms/step - loss: 0.0780 - accuracy: 0.9754 - val_loss: 0.4861 - val_accuracy: 0.8652\n",
      "Epoch 15/20\n",
      "157/157 [==============================] - 29s 184ms/step - loss: 0.0643 - accuracy: 0.9801 - val_loss: 0.5116 - val_accuracy: 0.8604\n",
      "Epoch 16/20\n",
      "157/157 [==============================] - 30s 191ms/step - loss: 0.0547 - accuracy: 0.9827 - val_loss: 0.7493 - val_accuracy: 0.8116\n",
      "Epoch 17/20\n",
      "157/157 [==============================] - 29s 186ms/step - loss: 0.0509 - accuracy: 0.9837 - val_loss: 0.5856 - val_accuracy: 0.8540\n",
      "Epoch 18/20\n",
      "157/157 [==============================] - 28s 181ms/step - loss: 0.0428 - accuracy: 0.9863 - val_loss: 0.6264 - val_accuracy: 0.8616\n",
      "Epoch 19/20\n",
      "157/157 [==============================] - 28s 179ms/step - loss: 0.0378 - accuracy: 0.9886 - val_loss: 0.6309 - val_accuracy: 0.8590\n",
      "Epoch 20/20\n",
      "157/157 [==============================] - 28s 178ms/step - loss: 0.0305 - accuracy: 0.9908 - val_loss: 0.7196 - val_accuracy: 0.8552\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.7676 - accuracy: 0.8465\n",
      "Test Loss: 0.7676042318344116, Test Accuracy: 0.8464800119400024\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Setting up parameters\n",
    "vocab_size = 10000  # Size of vocabulary\n",
    "max_len = 200  # Maximum length of sequences\n",
    "embedding_dim = 128  # Dimension of word embeddings\n",
    "\n",
    "# Loading the IMDB dataset\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# Preprocessing the data\n",
    "train_data = pad_sequences(train_data, maxlen=max_len)\n",
    "test_data = pad_sequences(test_data, maxlen=max_len)\n",
    "\n",
    "# Defining the model architecture\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compiling the model with RMSprop optimizer\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(train_data, train_labels, epochs=20, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = model.evaluate(test_data, test_labels)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
